{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/Arshad221b/Sign-Language-Recognition/blob/master/ASLwithCNN\n",
    "- https://data-flair.training/blogs/sign-language-recognition-python-ml-opencv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import Conv2D,MaxPooling2D, Dense,Flatten, Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "training_set = pd.read_csv('./data/sign_mnist_train.csv')\n",
    "test_set = pd.read_csv('./data/sign_mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (28, 28)\n",
    "CLASSES = dict(zip(range(0, 26), list(map(chr, range(97, 123))))) # labels from A to Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    global SIZE\n",
    "    labels, images = data[0:, 0], np.array([np.reshape(i[1:], SIZE) for i in data])\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocess_data(training_set.to_numpy())\n",
    "X_test, y_test = preprocess_data(test_set.to_numpy())\n",
    "\n",
    "y_train = np.array(y_train).reshape(-1)\n",
    "y_test = np.array(y_test).reshape(-1)\n",
    "\n",
    "y_train = np.eye(len(CLASSES))[y_train]\n",
    "y_test = np.eye(len(CLASSES))[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27455, 28, 28), (27455, 26), (7172, 28, 28), (7172, 26))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(filters=8, kernel_size=(3,3),strides=(1,1),padding='same',input_shape=(28,28,1),activation='relu', data_format='channels_last'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Conv2D(filters=16, kernel_size=(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(MaxPooling2D(pool_size=(4,4)))\n",
    "classifier.add(Dense(128, activation='relu'))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(26, activation='softmax'))\n",
    "classifier.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "275/275 [==============================] - 16s 42ms/step - loss: 2.6517 - accuracy: 0.4597\n",
      "Epoch 2/50\n",
      "275/275 [==============================] - 11s 42ms/step - loss: 0.5340 - accuracy: 0.8169\n",
      "Epoch 3/50\n",
      "275/275 [==============================] - 11s 40ms/step - loss: 0.3334 - accuracy: 0.8831\n",
      "Epoch 4/50\n",
      "275/275 [==============================] - 11s 40ms/step - loss: 0.2488 - accuracy: 0.9127\n",
      "Epoch 5/50\n",
      "275/275 [==============================] - 11s 40ms/step - loss: 0.1946 - accuracy: 0.9323\n",
      "Epoch 6/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.1688 - accuracy: 0.9425\n",
      "Epoch 7/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.1397 - accuracy: 0.9529\n",
      "Epoch 8/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.1295 - accuracy: 0.9557\n",
      "Epoch 9/50\n",
      "275/275 [==============================] - 11s 42ms/step - loss: 0.1159 - accuracy: 0.9610\n",
      "Epoch 10/50\n",
      "275/275 [==============================] - 11s 40ms/step - loss: 0.1043 - accuracy: 0.9657\n",
      "Epoch 11/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0998 - accuracy: 0.9661\n",
      "Epoch 12/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0957 - accuracy: 0.9679\n",
      "Epoch 13/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0828 - accuracy: 0.9723\n",
      "Epoch 14/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0788 - accuracy: 0.9726\n",
      "Epoch 15/50\n",
      "275/275 [==============================] - 12s 42ms/step - loss: 0.0743 - accuracy: 0.9745\n",
      "Epoch 16/50\n",
      "275/275 [==============================] - 12s 42ms/step - loss: 0.0715 - accuracy: 0.9768\n",
      "Epoch 17/50\n",
      "275/275 [==============================] - 12s 42ms/step - loss: 0.0717 - accuracy: 0.9755\n",
      "Epoch 18/50\n",
      "275/275 [==============================] - 12s 43ms/step - loss: 0.0662 - accuracy: 0.9768\n",
      "Epoch 19/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0610 - accuracy: 0.9787\n",
      "Epoch 20/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0637 - accuracy: 0.9789\n",
      "Epoch 21/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0587 - accuracy: 0.9803\n",
      "Epoch 22/50\n",
      "275/275 [==============================] - 11s 40ms/step - loss: 0.0594 - accuracy: 0.9807\n",
      "Epoch 23/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0522 - accuracy: 0.9823\n",
      "Epoch 24/50\n",
      "275/275 [==============================] - 11s 42ms/step - loss: 0.0522 - accuracy: 0.9833\n",
      "Epoch 25/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0495 - accuracy: 0.9829\n",
      "Epoch 26/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0504 - accuracy: 0.9826\n",
      "Epoch 27/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0556 - accuracy: 0.9823\n",
      "Epoch 28/50\n",
      "275/275 [==============================] - 11s 42ms/step - loss: 0.0561 - accuracy: 0.9820\n",
      "Epoch 29/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0443 - accuracy: 0.9856\n",
      "Epoch 30/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0455 - accuracy: 0.9854\n",
      "Epoch 31/50\n",
      "275/275 [==============================] - 12s 43ms/step - loss: 0.0480 - accuracy: 0.9850\n",
      "Epoch 32/50\n",
      "275/275 [==============================] - 11s 42ms/step - loss: 0.0494 - accuracy: 0.9839\n",
      "Epoch 33/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0445 - accuracy: 0.9857\n",
      "Epoch 34/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0521 - accuracy: 0.9836\n",
      "Epoch 35/50\n",
      "275/275 [==============================] - 11s 40ms/step - loss: 0.0384 - accuracy: 0.9878\n",
      "Epoch 36/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0324 - accuracy: 0.9896\n",
      "Epoch 37/50\n",
      "275/275 [==============================] - 11s 40ms/step - loss: 0.0357 - accuracy: 0.9886\n",
      "Epoch 38/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0400 - accuracy: 0.9876\n",
      "Epoch 39/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0355 - accuracy: 0.9891\n",
      "Epoch 40/50\n",
      "275/275 [==============================] - 11s 42ms/step - loss: 0.0344 - accuracy: 0.9889\n",
      "Epoch 41/50\n",
      "275/275 [==============================] - 12s 44ms/step - loss: 0.0360 - accuracy: 0.9887\n",
      "Epoch 42/50\n",
      "275/275 [==============================] - 12s 42ms/step - loss: 0.0436 - accuracy: 0.9864\n",
      "Epoch 43/50\n",
      "275/275 [==============================] - 12s 42ms/step - loss: 0.0364 - accuracy: 0.9887\n",
      "Epoch 44/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0377 - accuracy: 0.9874\n",
      "Epoch 45/50\n",
      "275/275 [==============================] - 12s 42ms/step - loss: 0.0355 - accuracy: 0.9892\n",
      "Epoch 46/50\n",
      "275/275 [==============================] - 12s 42ms/step - loss: 0.0359 - accuracy: 0.9896\n",
      "Epoch 47/50\n",
      "275/275 [==============================] - 11s 41ms/step - loss: 0.0337 - accuracy: 0.9898\n",
      "Epoch 48/50\n",
      "275/275 [==============================] - 12s 42ms/step - loss: 0.0347 - accuracy: 0.9898\n",
      "Epoch 49/50\n",
      "275/275 [==============================] - 12s 43ms/step - loss: 0.0320 - accuracy: 0.9901\n",
      "Epoch 50/50\n",
      "275/275 [==============================] - 12s 43ms/step - loss: 0.0339 - accuracy: 0.9897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e8c92221f0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.fit(X_train, y_train, epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 2s 7ms/step - loss: 0.2886 - accuracy: 0.9030\n",
      "Accuracy:  90.2955949306488\n"
     ]
    }
   ],
   "source": [
    "accuracy = classifier.evaluate(x=X_test,y=y_test,batch_size=32)\n",
    "print(\"Accuracy: \",accuracy[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.19.2-cp38-cp38-win_amd64.whl (12.6 MB)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2022.5.4-py3-none-any.whl (195 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell-2019\\anaconda3\\envs\\ml\\lib\\site-packages (from scikit-image) (21.3)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.3.0-cp38-cp38-win_amd64.whl (4.2 MB)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\dell-2019\\anaconda3\\envs\\ml\\lib\\site-packages (from scikit-image) (1.6.2)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\dell-2019\\anaconda3\\envs\\ml\\lib\\site-packages (from scikit-image) (9.0.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\dell-2019\\anaconda3\\envs\\ml\\lib\\site-packages (from scikit-image) (1.22.3)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\dell-2019\\anaconda3\\envs\\ml\\lib\\site-packages (from scikit-image) (2.7.1)\n",
      "Collecting imageio>=2.4.1\n",
      "  Downloading imageio-2.19.1-py3-none-any.whl (3.4 MB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dell-2019\\anaconda3\\envs\\ml\\lib\\site-packages (from packaging>=20.0->scikit-image) (3.0.4)\n",
      "Installing collected packages: tifffile, PyWavelets, imageio, scikit-image\n",
      "Successfully installed PyWavelets-1.3.0 imageio-2.19.1 scikit-image-0.19.2 tifffile-2022.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize, pyramid_reduce\n",
    "\n",
    "def get_square(image, square_size):\n",
    "    height, width = image.shape    \n",
    "    if(height > width):\n",
    "      differ = height\n",
    "    else:\n",
    "      differ = width\n",
    "    differ += 4\n",
    "\n",
    "\n",
    "    mask = np.zeros((differ, differ), dtype = \"uint8\")\n",
    "\n",
    "    x_pos = int((differ - width) / 2)\n",
    "    y_pos = int((differ - height) / 2)\n",
    "\n",
    "   \n",
    "    mask[y_pos: y_pos + height, x_pos: x_pos + width] = image[0: height, 0: width]\n",
    "\n",
    " \n",
    "    if differ / square_size > 1:\n",
    "      mask = pyramid_reduce(mask, differ / square_size)\n",
    "    else:\n",
    "      mask = cv2.resize(mask, (square_size, square_size), interpolation = cv2.INTER_AREA)\n",
    "    return mask\n",
    "\n",
    "def crop_image(image, x, y, width, height):\n",
    "    return image[y:y + height, x:x + width]\n",
    "\n",
    "def keras_process_image(img):\n",
    "    \n",
    "    image_x = 28\n",
    "    image_y = 28\n",
    "    #img = cv2.resize(img, (28,28), interpolation = cv2.INTER_AREA)\n",
    "    img = get_square(img, 28)\n",
    "    img = np.reshape(img, (image_x, image_y))\n",
    "    \n",
    "    \n",
    "    return img\n",
    "\n",
    "def keras_predict(model, image):\n",
    "    data = np.asarray(image, dtype=\"int32\")\n",
    "    \n",
    "    pred_probab = model.predict(data)[0]\n",
    "    pred_class = list(pred_probab).index(max(pred_probab))\n",
    "    return max(pred_probab), pred_class\n",
    "\n",
    "def get_predicted_class(model, img):\n",
    "    # img = cv.resize(img, SIZE)\n",
    "    prediction = model.predict(np.array([img]))\n",
    "    return CLASSES[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "while True:  \n",
    "    cam_capture = cv2.VideoCapture(0)\n",
    "    _, image_frame = cam_capture.read()  \n",
    "    # Select ROI\n",
    "    im2 = crop_image(image_frame, 300,300,300,300)\n",
    "    image_grayscale = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "    image_grayscale_blurred = cv2.GaussianBlur(image_grayscale, (15,15), 0)\n",
    "\n",
    "\n",
    "    #resized_img = image_resize(image_grayscale_blurred, width = 28, height = 28, inter = cv2.INTER_AREA) \n",
    "    #resized_img = keras_process_image(image_grayscale_blurred)\n",
    "    resized_img = cv2.resize(image_grayscale_blurred,(28,28))\n",
    "    #ar = np.array(resized_img)\n",
    "    # ar = resized_img.reshape(1,784)\n",
    "\n",
    "    # pred_probab, pred_class = keras_predict(classifier, resized_img)\n",
    "    # print(pred_class, pred_probab)\n",
    " \n",
    "    # Display cropped image\n",
    "    cv2.imshow(\"Image2\",im2)\n",
    "    cv2.imshow(\"Image4\",resized_img)\n",
    "    cv2.imshow(\"Image3\",image_grayscale_blurred)\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "cam_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(classifier, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\dell-2019\\anaconda3\\envs\\ml\\lib\\site-packages (from pydot) (3.0.4)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.2\n",
      "Requirement already satisfied: pydot in c:\\users\\dell-2019\\anaconda3\\envs\\ml\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\dell-2019\\anaconda3\\envs\\ml\\lib\\site-packages (from pydot) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !choco install graphviz\n",
    "%pip install pydot"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ce70e7fb09d93846fb9da31c8ffeaa9e6be9e848f96b1b1e309019ff423a0ff"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
